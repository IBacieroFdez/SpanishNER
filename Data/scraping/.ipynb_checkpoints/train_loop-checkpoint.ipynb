{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bloque de código para el entrenamiento del módulo NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Importación de las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import xml.etree.ElementTree as ET              #Estos tres primeros paquetes servirán para adaptar el formato \n",
    "import pandas as pd                             #de salida de los textos anotados al formato que me requiere SpaCy\n",
    "\n",
    "import plac                                     #Estos paquetes son propios del modelo de SpaCy\n",
    "import random                                   #Ayudará a aleatorizar los textos de entrenamiento\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.pipeline import EntityRuler\n",
    "import datetime as dt                           #Ayuda a saber con detalle el tiempo de procesamiento por cada iteracion del entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Carga de los textos de entrenamiento \n",
    "\n",
    "    Iteracion correspondiente al fold_8 en el entrenamiento de los modelos k_10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data \n",
    "TRAIN_DATA=[]\n",
    "\n",
    "#Bucle for con el que iterar por todos los textos de entrenamiento. \n",
    "\n",
    "for tn in range (1,153):  \n",
    "    \n",
    "    #Introducimos el path del doc.xml \n",
    "    anot_path = '/Users/ignaciobacierofernandez/Desktop/TFG/NLP/dataset_k10/fold_8/annot/a_fold8_'+str(tn)+'.xml' \n",
    "\n",
    "    #Inicialización de los objetos de la libreria ElementTree que parsearan las \n",
    "    #anotaciones previamente realizadas en GATE\n",
    "    \n",
    "    tree = ET.parse(anot_path)                    \n",
    "    root = tree.getroot()                         \n",
    "    \n",
    "     #Introducción de las anotaciones en un dataframe de columnas 'start', 'end' y 'type'\n",
    "        \n",
    "    df_cols = [\"Start\", \"End\", \"Type\"]           \n",
    "    rows = []\n",
    "    i=0\n",
    "    for node in root.iter('Annotation'):\n",
    "        s_Id = node.attrib.get(\"Id\")\n",
    "        s_type = node.attrib.get(\"Type\")\n",
    "        s_start = int(node.attrib.get(\"StartNode\"))\n",
    "        s_end = int(node.attrib.get(\"EndNode\"))\n",
    "        i+=1\n",
    "        \n",
    "        #Inicializo las columnas Type, Start & End del dataframe\n",
    "        rows.append({\"Start\": s_start, \"End\": s_end, \"Type\": s_type})   \n",
    "\n",
    "    out_df = pd.DataFrame(rows, columns = df_cols)\n",
    "    \n",
    "    #Con estas lineas evito cargar al modelo ciertos tipos de etiquetas (para los \"modelo_sm\")\n",
    "    \n",
    "    out_df = out_df[out_df.Type != 'paragraph']                  #Borrar etiquetado de las etiquetas pertinentes.\n",
    "    out_df = out_df[out_df.Type != 'SPORT']\n",
    "    out_df = out_df[out_df.Type != 'MONEY']\n",
    "    out_df = out_df[out_df.Type != 'PRODUCT']\n",
    "    out_df = out_df[out_df.Type != 'MEDTERM']\n",
    "    out_df = out_df[out_df.Type != 'JOB']\n",
    "    out_df = out_df[out_df.Type != 'EVENT']\n",
    "    out_df = out_df[out_df.Type != 'LANG']\n",
    "    out_df = out_df[out_df.Type != 'LOC']\n",
    "    out_df = out_df[out_df.Type != 'PERCENT']\n",
    "    out_df = out_df[out_df.Type != 'PHONE']\n",
    "    \n",
    "    records = out_df.to_records(index=False)\n",
    "    annot = list(records)                                        \n",
    "\n",
    "    #Introducimos el path del doc.txt\n",
    "    text_path = '/Users/ignaciobacierofernandez/Desktop/TFG/NLP/dataset_k10/fold_8/txt/t_fold8_'+str(tn)+\".txt\"\n",
    "    texto =open(text_path, \"r\")                                         \n",
    "    texto =texto.read() \n",
    "    texto = texto.replace('\\n','')                        #Para evitar que me salgan etiquetas '\\n' en el texto\n",
    "    texto = texto.replace('\\t','')                              \n",
    "    TRAIN_DATA_min = (texto, {'entities':annot})          #Cada uno de los textos de entrenamiento + anotaciones\n",
    "   \n",
    "    #print(texto)\n",
    "    TRAIN_DATA.append(TRAIN_DATA_min)                            \n",
    "    \n",
    "#Descomentar la siguiente linea si queremos ver el formato del training data\n",
    "\n",
    "print(TRAIN_DATA)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Función de entrenamiento\n",
    "    \n",
    "    Consiste en un bucle que itera por cada uno de los ejemplos que se le proporciona por fase de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model= None, new_model_name = None, output_dir = None, n_iter= None):  \n",
    "   \n",
    "    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"es\")  # create blank Language class\n",
    "        print(\"Created blank 'es' model\")\n",
    "\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    \n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "        \n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        i=0\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "            i+=1\n",
    "            #print(ent[2],i, end=' ')                   #Si quiero ver todas las entidades \n",
    "    pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "    with nlp.disable_pipes(*other_pipes):               # solo queremos entrenar el modulo NER\n",
    "        \n",
    "        # reset and initialize the weights randomly – but only if we're # training a new model\n",
    "        if model is None:\n",
    "            optimizer = nlp.begin_training()\n",
    "        else:\n",
    "            optimizer = nlp.resume_training()\n",
    "        for itn in range(n_iter):\n",
    "            \n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            \n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))    #parametros recomendados en NER\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                    texts,                              # batch of texts\n",
    "                    annotations,                        # batch of annotations\n",
    "                    drop= 0.15,                         # dropout - make it harder to memorise data \n",
    "                    sgd=optimizer,                      # callable to update weights\n",
    "                    losses=losses)\n",
    "\n",
    "            print(f\"Losses at iteration {itn} - {dt.datetime.now()}\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    for text, _ in TRAIN_DATA:\n",
    "        doc = nlp(text)\n",
    "        displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.meta[\"name\"] = new_model_name\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)  # rename model\n",
    "\n",
    "    # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        for text, _ in TRAIN_DATA:\n",
    "            doc = nlp2(text)\n",
    "            print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Llamada propiamente dicha a la funcion de entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '/Users/ignaciobacierofernandez/Desktop/TFG/NLP/dataset_k10/modelos/modelo_k10_sm_dr_1/it_7'  #es_core_news_md\n",
    "new_model = \"es_14492_news_mod_k10_sm_dr_1_it_8\"\n",
    "output_dir = '/Users/ignaciobacierofernandez/Desktop/TFG/NLP/dataset_k10/modelos/modelo_k10_sm_dr_1/it_8'     \n",
    "n_it = 50\n",
    "\n",
    "#Llamada a la funcion training_loop\n",
    "\n",
    "training_loop( model, new_model, output_dir, n_it)     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
